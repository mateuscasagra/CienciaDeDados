import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    precision_recall_curve, average_precision_score
)

class MetricasAvaliacao:
    def __init__(self):
        self.resultados = {}
        
    def calcular_metricas_basicas(self, y_true, y_pred, nome_modelo):
        """Calcula m√©tricas b√°sicas de classifica√ß√£o"""
        metricas = {
            'acuracia': accuracy_score(y_true, y_pred),
            'precisao': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'matriz_confusao': confusion_matrix(y_true, y_pred)
        }
        
        self.resultados[nome_modelo] = metricas
        return metricas
    
    def calcular_metricas_avancadas(self, y_true, y_pred_proba, nome_modelo):
        """Calcula m√©tricas avan√ßadas usando probabilidades"""
        if nome_modelo not in self.resultados:
            self.resultados[nome_modelo] = {}
        
        # ROC AUC
        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        
        # Precision-Recall AUC
        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
        pr_auc = average_precision_score(y_true, y_pred_proba)
        
        self.resultados[nome_modelo].update({
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'fpr': fpr,
            'tpr': tpr,
            'precision_curve': precision,
            'recall_curve': recall
        })
        
        return roc_auc, pr_auc
    
    def gerar_matriz_confusao_detalhada(self, y_true, y_pred, nome_modelo, labels=['N√£o Fatal', 'Fatal']):
        """Gera matriz de confus√£o detalhada com interpreta√ß√£o"""
        cm = confusion_matrix(y_true, y_pred)
        
        # Calcula m√©tricas derivadas da matriz
        tn, fp, fn, tp = cm.ravel()
        
        detalhes = {
            'verdadeiros_negativos': tn,
            'falsos_positivos': fp,
            'falsos_negativos': fn,
            'verdadeiros_positivos': tp,
            'total_casos': len(y_true),
            'casos_positivos': sum(y_true),
            'casos_negativos': len(y_true) - sum(y_true),
            'taxa_verdadeiros_positivos': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'taxa_falsos_positivos': fp / (fp + tn) if (fp + tn) > 0 else 0,
            'especificidade': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'sensibilidade': tp / (tp + fn) if (tp + fn) > 0 else 0
        }
        
        return cm, detalhes
    
    def plotar_matriz_confusao(self, y_true, y_pred, nome_modelo, labels=['N√£o Fatal', 'Fatal']):
        """Plota matriz de confus√£o com detalhes"""
        cm, detalhes = self.gerar_matriz_confusao_detalhada(y_true, y_pred, nome_modelo, labels)
        
        plt.figure(figsize=(10, 8))
        
        # Subplot 1: Matriz de confus√£o
        plt.subplot(2, 2, 1)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=labels, yticklabels=labels)
        plt.title(f'Matriz de Confus√£o - {nome_modelo}')
        plt.xlabel('Predito')
        plt.ylabel('Real')
        
        # Subplot 2: M√©tricas em barras
        plt.subplot(2, 2, 2)
        metricas_nomes = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'Especificidade']
        metricas_valores = [
            self.resultados[nome_modelo]['acuracia'],
            self.resultados[nome_modelo]['precisao'],
            self.resultados[nome_modelo]['recall'],
            self.resultados[nome_modelo]['f1_score'],
            detalhes['especificidade']
        ]
        
        cores = ['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum']
        bars = plt.bar(metricas_nomes, metricas_valores, color=cores)
        plt.title('M√©tricas de Performance')
        plt.ylabel('Score')
        plt.ylim(0, 1)
        
        # Adiciona valores nas barras
        for bar, valor in zip(bars, metricas_valores):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{valor:.3f}', ha='center', va='bottom')
        
        plt.xticks(rotation=45)
        
        # Subplot 3: Distribui√ß√£o de casos
        plt.subplot(2, 2, 3)
        casos_reais = ['N√£o Fatal', 'Fatal']
        casos_valores = [detalhes['casos_negativos'], detalhes['casos_positivos']]
        plt.pie(casos_valores, labels=casos_reais, autopct='%1.1f%%', startangle=90)
        plt.title('Distribui√ß√£o Real dos Casos')
        
        # Subplot 4: Detalhes da matriz
        plt.subplot(2, 2, 4)
        plt.axis('off')
        texto_detalhes = f"""
        DETALHES DA CLASSIFICA√á√ÉO:
        
        Total de Casos: {detalhes['total_casos']}
        Casos Fatais: {detalhes['casos_positivos']} ({detalhes['casos_positivos']/detalhes['total_casos']*100:.1f}%)
        Casos N√£o Fatais: {detalhes['casos_negativos']} ({detalhes['casos_negativos']/detalhes['total_casos']*100:.1f}%)
        
        MATRIZ DE CONFUS√ÉO:
        Verdadeiros Positivos: {detalhes['verdadeiros_positivos']}
        Verdadeiros Negativos: {detalhes['verdadeiros_negativos']}
        Falsos Positivos: {detalhes['falsos_positivos']}
        Falsos Negativos: {detalhes['falsos_negativos']}
        
        TAXAS:
        Sensibilidade (Recall): {detalhes['sensibilidade']:.3f}
        Especificidade: {detalhes['especificidade']:.3f}
        Taxa de Falsos Positivos: {detalhes['taxa_falsos_positivos']:.3f}
        """
        
        plt.text(0.1, 0.9, texto_detalhes, transform=plt.gca().transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        plt.savefig(f'matriz_confusao_{nome_modelo.lower()}.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        return cm, detalhes
    
    def comparar_modelos(self, modelos_resultados):
        """Compara m√∫ltiplos modelos e gera ranking"""
        df_comparacao = pd.DataFrame()
        
        for nome, resultados in modelos_resultados.items():
            df_comparacao[nome] = [
                resultados['acuracia'],
                resultados['precisao'],
                resultados['recall'],
                resultados['f1_score']
            ]
        
        df_comparacao.index = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']
        
        # Plota compara√ß√£o
        plt.figure(figsize=(12, 8))
        
        # Subplot 1: Heatmap de compara√ß√£o
        plt.subplot(2, 2, 1)
        sns.heatmap(df_comparacao, annot=True, fmt='.3f', cmap='RdYlGn', 
                   cbar_kws={'label': 'Score'})
        plt.title('Compara√ß√£o de M√©tricas entre Modelos')
        
        # Subplot 2: Ranking por F1-Score
        plt.subplot(2, 2, 2)
        f1_scores = {nome: res['f1_score'] for nome, res in modelos_resultados.items()}
        f1_sorted = sorted(f1_scores.items(), key=lambda x: x[1], reverse=True)
        
        nomes = [nome.replace('_', ' ').title() for nome, _ in f1_sorted]
        scores = [score for _, score in f1_sorted]
        cores = ['gold', 'silver', '#CD7F32', 'lightcoral'][:len(nomes)]
        
        bars = plt.barh(nomes, scores, color=cores)
        plt.title('Ranking por F1-Score')
        plt.xlabel('F1-Score')
        
        for bar, score in zip(bars, scores):
            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.3f}', va='center')
        
        # Subplot 3: Radar chart
        plt.subplot(2, 2, 3, projection='polar')
        
        # Pega o melhor modelo para o radar
        melhor_modelo = f1_sorted[0][0]
        melhor_resultados = modelos_resultados[melhor_modelo]
        
        categorias = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']
        valores = [
            melhor_resultados['acuracia'],
            melhor_resultados['precisao'],
            melhor_resultados['recall'],
            melhor_resultados['f1_score']
        ]
        
        angulos = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()
        valores += valores[:1]  # Fecha o c√≠rculo
        angulos += angulos[:1]
        
        plt.plot(angulos, valores, 'o-', linewidth=2, label=melhor_modelo.replace('_', ' ').title())
        plt.fill(angulos, valores, alpha=0.25)
        plt.xticks(angulos[:-1], categorias)
        plt.ylim(0, 1)
        plt.title(f'Perfil do Melhor Modelo\n({melhor_modelo.replace("_", " ").title()})')
        
        # Subplot 4: Resumo estat√≠stico
        plt.subplot(2, 2, 4)
        plt.axis('off')
        
        # Calcula estat√≠sticas
        media_f1 = np.mean([res['f1_score'] for res in modelos_resultados.values()])
        std_f1 = np.std([res['f1_score'] for res in modelos_resultados.values()])
        melhor_f1 = max([res['f1_score'] for res in modelos_resultados.values()])
        pior_f1 = min([res['f1_score'] for res in modelos_resultados.values()])
        
        resumo_texto = f"""
        RESUMO ESTAT√çSTICO (F1-Score):
        
        üèÜ Melhor Modelo: {f1_sorted[0][0].replace('_', ' ').title()}
        üìä Melhor F1-Score: {melhor_f1:.3f}
        üìâ Pior F1-Score: {pior_f1:.3f}
        üìà M√©dia F1-Score: {media_f1:.3f}
        üìè Desvio Padr√£o: {std_f1:.3f}
        
        RECOMENDA√á√ÉO:
        Usar {f1_sorted[0][0].replace('_', ' ')} para produ√ß√£o
        
        DIFEREN√áA:
        {((melhor_f1 - pior_f1) * 100):.1f}% entre melhor e pior modelo
        """
        
        plt.text(0.1, 0.9, resumo_texto, transform=plt.gca().transAxes,
                fontsize=11, verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        plt.savefig('comparacao_modelos.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # AGORA GERA CADA GR√ÅFICO INDIVIDUALMENTE
        self.gerar_graficos_metricas_individuais(modelos_resultados, df_comparacao, f1_sorted)
        
        return df_comparacao, f1_sorted
    
    def gerar_graficos_metricas_individuais(self, modelos_resultados, df_comparacao, f1_sorted):
        """Gera cada gr√°fico de m√©tricas individualmente para uso nos slides"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np
        
        # 1. Heatmap de compara√ß√£o de m√©tricas
        plt.figure(figsize=(12, 8))
        sns.heatmap(df_comparacao, annot=True, fmt='.3f', cmap='RdYlGn', 
                   cbar_kws={'label': 'Score'}, linewidths=0.5, linecolor='gray')
        plt.title('Compara√ß√£o de M√©tricas entre Modelos', fontsize=14, fontweight='bold')
        plt.xlabel('Modelos', fontsize=12, fontweight='bold')
        plt.ylabel('M√©tricas', fontsize=12, fontweight='bold')
        plt.tight_layout()
        plt.savefig('src/machine_learning/heatmap_comparacao_metricas.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Ranking por F1-Score
        plt.figure(figsize=(10, 8))
        nomes = [nome.replace('_', ' ').title() for nome, _ in f1_sorted]
        scores = [score for _, score in f1_sorted]
        cores = ['gold', 'silver', '#CD7F32', 'lightcoral'][:len(nomes)]
        
        bars = plt.barh(nomes, scores, color=cores)
        plt.title('Ranking dos Modelos por F1-Score', fontsize=14, fontweight='bold')
        plt.xlabel('F1-Score', fontsize=12, fontweight='bold')
        plt.ylabel('Modelos', fontsize=12, fontweight='bold')
        plt.xlim(0, 1)
        
        for bar, score in zip(bars, scores):
            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                    f'{score:.3f}', va='center', fontsize=11, fontweight='bold')
        
        plt.gca().invert_yaxis()
        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig('src/machine_learning/ranking_f1_score.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. Radar chart do melhor modelo
        melhor_modelo = f1_sorted[0][0]
        melhor_resultados = modelos_resultados[melhor_modelo]
        
        categorias = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']
        valores = [
            melhor_resultados['acuracia'],
            melhor_resultados['precisao'],
            melhor_resultados['recall'],
            melhor_resultados['f1_score']
        ]
        
        angulos = np.linspace(0, 2 * np.pi, len(categorias), endpoint=False).tolist()
        valores += valores[:1]  # Fecha o c√≠rculo
        angulos += angulos[:1]
        
        plt.figure(figsize=(10, 10))
        ax = plt.subplot(111, projection='polar')
        ax.plot(angulos, valores, 'o-', linewidth=3, label=melhor_modelo.replace('_', ' ').title())
        ax.fill(angulos, valores, alpha=0.25)
        ax.set_xticks(angulos[:-1])
        ax.set_xticklabels(categorias, fontsize=12, fontweight='bold')
        ax.set_ylim(0, 1)
        ax.set_title(f'Perfil do Melhor Modelo\n({melhor_modelo.replace("_", " ").title()})', 
                    fontsize=14, fontweight='bold', pad=20)
        ax.grid(True)
        plt.tight_layout()
        plt.savefig('src/machine_learning/radar_chart_melhor_modelo.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def gerar_relatorio_metricas(self, modelos_resultados):
        """Gera relat√≥rio completo das m√©tricas"""
        relatorio = "=== RELAT√ìRIO COMPLETO DE M√âTRICAS ===\n\n"
        
        # Ranking
        f1_scores = {nome: res['f1_score'] for nome, res in modelos_resultados.items()}
        ranking = sorted(f1_scores.items(), key=lambda x: x[1], reverse=True)
        
        relatorio += "üèÜ RANKING DOS MODELOS (F1-Score):\n"
        for i, (nome, score) in enumerate(ranking, 1):
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "üìä"
            relatorio += f"{emoji} {i}¬∫ lugar: {nome.replace('_', ' ').title()} - {score:.3f}\n"
        
        relatorio += "\n" + "="*60 + "\n\n"
        
        # Detalhes por modelo
        for nome, resultados in modelos_resultados.items():
            relatorio += f"üìã MODELO: {nome.replace('_', ' ').upper()}\n"
            relatorio += f"   Acur√°cia: {resultados['acuracia']:.3f} ({resultados['acuracia']*100:.1f}%)\n"
            relatorio += f"   Precis√£o: {resultados['precisao']:.3f} ({resultados['precisao']*100:.1f}%)\n"
            relatorio += f"   Recall: {resultados['recall']:.3f} ({resultados['recall']*100:.1f}%)\n"
            relatorio += f"   F1-Score: {resultados['f1_score']:.3f} ({resultados['f1_score']*100:.1f}%)\n"
            
            # Interpreta√ß√£o da matriz de confus√£o
            cm = resultados['matriz_confusao']
            tn, fp, fn, tp = cm.ravel()
            
            relatorio += f"   \n   üìä MATRIZ DE CONFUS√ÉO:\n"
            relatorio += f"   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n"
            relatorio += f"   ‚îÇ TN: {tn:7d} ‚îÇ FP: {fp:7d} ‚îÇ\n"
            relatorio += f"   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n"
            relatorio += f"   ‚îÇ FN: {fn:7d} ‚îÇ TP: {tp:7d} ‚îÇ\n"
            relatorio += f"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            
            # Interpreta√ß√£o pr√°tica
            total = tn + fp + fn + tp
            relatorio += f"   \n   üí° INTERPRETA√á√ÉO PR√ÅTICA:\n"
            relatorio += f"   ‚Ä¢ De {total} casos analisados:\n"
            relatorio += f"     - {tp} acidentes fatais identificados corretamente\n"
            relatorio += f"     - {tn} acidentes n√£o fatais identificados corretamente\n"
            relatorio += f"     - {fp} falsos alarmes (predito fatal, mas n√£o foi)\n"
            relatorio += f"     - {fn} casos perdidos (era fatal, mas n√£o foi detectado)\n\n"
        
        # Recomenda√ß√µes
        melhor_modelo = ranking[0][0]
        melhor_score = ranking[0][1]
        
        relatorio += "üéØ RECOMENDA√á√ïES:\n"
        relatorio += f"‚Ä¢ Implementar o modelo {melhor_modelo.replace('_', ' ')} em produ√ß√£o\n"
        relatorio += f"‚Ä¢ F1-Score de {melhor_score:.3f} indica boa capacidade preditiva\n"
        relatorio += f"‚Ä¢ Monitorar continuamente a performance do modelo\n"
        relatorio += f"‚Ä¢ Considerar retreinamento com novos dados periodicamente\n"
        
        if melhor_score < 0.7:
            relatorio += f"‚ö†Ô∏è  ATEN√á√ÉO: F1-Score abaixo de 0.7 - considerar:\n"
            relatorio += f"   ‚Ä¢ Coleta de mais dados\n"
            relatorio += f"   ‚Ä¢ Engenharia de features\n"
            relatorio += f"   ‚Ä¢ Ajuste de hiperpar√¢metros\n"
        
        return relatorio